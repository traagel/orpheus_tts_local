from llama_cpp import Llama
import os
import sys
import requests
import json
import time
import wave
import numpy as np
import sounddevice as sd
import argparse
import threading
import queue
import asyncio

# LM Studio API settings
API_URL = "http://127.0.0.1:8080/v1/completions"
HEADERS = {"Content-Type": "application/json"}

# Model parameters
MAX_TOKENS = 4096
TEMPERATURE = 0.6
TOP_P = 0.9
REPETITION_PENALTY = 1.1
SAMPLE_RATE = 24000  # SNAC model uses 24kHz

# Available voices based on the Orpheus-TTS repository
AVAILABLE_VOICES = ["tara", "leah", "jess", "leo", "dan", "mia", "zac", "zoe"]
DEFAULT_VOICE = "tara"  # Best voice according to documentation

# Special token IDs for Orpheus model
START_TOKEN_ID = 128259
END_TOKEN_IDS = [128009, 128260, 128261, 128257]
CUSTOM_TOKEN_PREFIX = "<custom_token_"


def format_prompt(prompt, voice=DEFAULT_VOICE):
    if voice not in AVAILABLE_VOICES:
        print(
            f"Warning: Voice '{voice}' not recognized. Using '{DEFAULT_VOICE}' instead."
        )
        voice = DEFAULT_VOICE
    formatted_prompt = f"{voice}: {prompt}"
    return f"<|audio|>{formatted_prompt}<|eot_id|>"


def generate_tokens_from_api(
    prompt,
    voice=DEFAULT_VOICE,
    temperature=TEMPERATURE,
    top_p=TOP_P,
    max_tokens=MAX_TOKENS,
    repetition_penalty=REPETITION_PENALTY,
    verbose=False,
):
    formatted_prompt = format_prompt(prompt, voice)
    if verbose:
        print(f"[PROMPT]: {formatted_prompt}")

    payload = {
        "prompt": formatted_prompt,
        "temperature": temperature,
        "top_p": top_p,
        "max_tokens": max_tokens,
        "stop": None,
        "stream": True,
        "repeat_penalty": repetition_penalty,
    }

    with requests.post(API_URL, headers=HEADERS, json=payload, stream=True) as response:
        if response.status_code != 200:
            raise RuntimeError(
                f"Error from llama-server: {response.status_code}, {response.text}"
            )

        for line in response.iter_lines(decode_unicode=True):
            if line.startswith("data: "):
                json_data = line[len("data: ") :]
                if json_data.strip() == "[DONE]":
                    break
                try:
                    data = json.loads(json_data)
                    token_text = data["choices"][0]["text"]
                    if verbose:
                        print(f"[TOKEN]: {token_text.strip()}")
                    yield token_text
                except Exception as e:
                    print(f"Error parsing streamed response: {e}")


def turn_token_into_id(token_string, index):
    token_string = token_string.strip()
    last_token_start = token_string.rfind(CUSTOM_TOKEN_PREFIX)
    if last_token_start == -1:
        return None
    last_token = token_string[last_token_start:]
    if last_token.startswith(CUSTOM_TOKEN_PREFIX) and last_token.endswith(">"):
        try:
            number_str = last_token[14:-1]
            token_id = int(number_str) - 10 - ((index % 7) * 4096)
            return token_id
        except ValueError:
            return None
    else:
        return None


def convert_to_audio(multiframe, count):
    from decoder import convert_to_audio as orpheus_convert_to_audio

    return orpheus_convert_to_audio(multiframe, count)


async def tokens_decoder(token_gen, verbose=False):
    buffer = []
    count = 0
    async for token_text in token_gen:
        token = turn_token_into_id(token_text, count)
        if token is not None and token > 0:
            buffer.append(token)
            count += 1
            if verbose:
                print(f"[ID]: {token}")
            if count % 7 == 0 and count > 27:
                buffer_to_proc = buffer[-28:]
                audio_samples = convert_to_audio(buffer_to_proc, count)
                if audio_samples is not None:
                    yield audio_samples


def tokens_decoder_sync(syn_token_gen, output_file=None, verbose=False):
    audio_queue = queue.Queue()
    audio_segments = []
    token_count = 0

    wav_file = None
    if output_file:
        os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
        wav_file = wave.open(output_file, "wb")
        wav_file.setnchannels(1)
        wav_file.setsampwidth(2)
        wav_file.setframerate(SAMPLE_RATE)

    async def async_token_gen():
        nonlocal token_count
        for token in syn_token_gen:
            token_count += 1
            yield token

    async def async_producer():
        async for audio_chunk in tokens_decoder(async_token_gen(), verbose=verbose):
            audio_queue.put(audio_chunk)
        audio_queue.put(None)

    def run_async():
        asyncio.run(async_producer())

    thread = threading.Thread(target=run_async)
    thread.start()

    while True:
        audio = audio_queue.get()
        if audio is None:
            break
        audio_segments.append(audio)
        if wav_file:
            wav_file.writeframes(audio)

    if wav_file:
        wav_file.close()

    thread.join()

    duration = sum([len(segment) // 2 for segment in audio_segments]) / SAMPLE_RATE

    print(f"Generated {len(audio_segments)} audio segments")
    print(f"Generated {duration:.2f} seconds of audio")
    print(f"Total tokens processed: {token_count}")

    return audio_segments


def generate_speech_from_api(
    prompt,
    voice=DEFAULT_VOICE,
    output_file=None,
    temperature=TEMPERATURE,
    top_p=TOP_P,
    max_tokens=MAX_TOKENS,
    repetition_penalty=REPETITION_PENALTY,
    verbose=False,
):
    return tokens_decoder_sync(
        generate_tokens_from_api(
            prompt=prompt,
            voice=voice,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            repetition_penalty=repetition_penalty,
            verbose=verbose,
        ),
        output_file=output_file,
        verbose=verbose,
    )


def list_available_voices():
    print("Available voices (in order of conversational realism):")
    for i, voice in enumerate(AVAILABLE_VOICES):
        marker = "â˜…" if voice == DEFAULT_VOICE else " "
        print(f"{marker} {voice}")
    print(f"\nDefault voice: {DEFAULT_VOICE}")
    print("\nAvailable emotion tags:")
    print("<laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>")


def main():
    parser = argparse.ArgumentParser(
        description="Orpheus Text-to-Speech using llama-server"
    )
    parser.add_argument("--text", type=str, help="Text to convert to speech")
    parser.add_argument("--file", type=str, help="Read input text from file")
    parser.add_argument(
        "--voice",
        type=str,
        default=DEFAULT_VOICE,
        help=f"Voice to use (default: {DEFAULT_VOICE})",
    )
    parser.add_argument("--output", type=str, help="Output WAV file path")
    parser.add_argument(
        "--list-voices", action="store_true", help="List available voices"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=TEMPERATURE,
        help="Temperature for generation",
    )
    parser.add_argument(
        "--top_p", type=float, default=TOP_P, help="Top-p sampling parameter"
    )
    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=REPETITION_PENALTY,
        help="Repetition penalty (>=1.1 required for stable generation)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print generated tokens and decoder debug info",
    )

    args = parser.parse_args()

    if args.list_voices:
        list_available_voices()
        return

    prompt = None
    if args.file:
        with open(args.file, "r", encoding="utf-8") as f:
            prompt = f.read().strip()
    elif args.text:
        prompt = args.text

    if not prompt:
        prompt = input("Enter text to synthesize: ")
        if not prompt:
            prompt = "Hello, I am Orpheus, an AI assistant with emotional speech capabilities."

    output_file = args.output
    if not output_file:
        os.makedirs("outputs", exist_ok=True)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"outputs/{args.voice}_{timestamp}.wav"
        print(f"No output file specified. Saving to {output_file}")

    start_time = time.time()
    audio_segments = generate_speech_from_api(
        prompt=prompt,
        voice=args.voice,
        temperature=args.temperature,
        top_p=args.top_p,
        repetition_penalty=args.repetition_penalty,
        output_file=output_file,
        verbose=args.verbose,
    )
    end_time = time.time()

    print(f"Speech generation completed in {end_time - start_time:.2f} seconds")
    print(f"Audio saved to {output_file}")


if __name__ == "__main__":
    main()

